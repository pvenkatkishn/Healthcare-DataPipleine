{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO38WcT30dKOIcj06NVqoz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvenkatkishn/Healthcare-DataPipleine/blob/main/Redshift_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xPE5zA2-VZx",
        "outputId": "e82e854f-146d-4763-ea3a-45ba85794e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vcfpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Rxt2LqGNnv",
        "outputId": "25581c1f-c654-4dfa-9d9d-41e38221a08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vcfpy\n",
            "  Downloading vcfpy-0.13.8.tar.gz (993 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/993.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/993.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m983.0/993.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.2/993.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pysam>=0.10.0 (from vcfpy)\n",
            "  Downloading pysam-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading pysam-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl (22.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: vcfpy\n",
            "  Building wheel for vcfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vcfpy: filename=vcfpy-0.13.8-py2.py3-none-any.whl size=34400 sha256=d22012d6ed7e21252a5495af9bd52b150bb9535f57f97c42fef738d15cac333b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/18/ae/1270a4895ffbb577ccf51ebe1396ce1dc6ce89f851079a2ec2\n",
            "Successfully built vcfpy\n",
            "Installing collected packages: pysam, vcfpy\n",
            "Successfully installed pysam-0.22.1 vcfpy-0.13.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vcfpy\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory where your .vcf.gz files are stored\n",
        "vcf_dir = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/test'\n",
        "output_dir = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/chr_agg_data.csv'\n",
        "\n",
        "# Define the function to process and aggregate a single .vcf.gz file\n",
        "def process_vcf(file_path):\n",
        "    reader = vcfpy.Reader.from_path(file_path)\n",
        "    records = []\n",
        "\n",
        "    for record in reader:\n",
        "        chrom = record.CHROM\n",
        "        pos = record.POS\n",
        "        ref = record.REF\n",
        "        alt = ','.join(str(a) for a in record.ALT)\n",
        "        allele_count = record.INFO.get('AC', [0])[0]\n",
        "        allele_freq = record.INFO.get('AF', [0])[0]\n",
        "\n",
        "        # Assuming you have metadata for gender and location\n",
        "        location_name = 'Unknown'  # Replace this with actual extraction if possible\n",
        "        gender = 'Unknown'  # Replace this with actual extraction if possible\n",
        "\n",
        "        records.append([chrom, pos, ref, alt, allele_count, allele_freq, location_name, gender])\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(records, columns=['CHROM', 'POS', 'REF', 'ALT', 'Allele_Count', 'Allele_Frequency', 'Location_Name', 'Gender'])\n",
        "\n",
        "    # Aggregate by Location_Name and Gender\n",
        "    aggregated_df = df.groupby(['Location_Name', 'Gender']).agg({\n",
        "        'Allele_Frequency': 'mean',\n",
        "        'Allele_Count': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "# Process each .vcf.gz file and save the output as a .csv file\n",
        "for file in os.listdir(vcf_dir):\n",
        "    if file.endswith('.vcf.gz'):\n",
        "        file_path = os.path.join(vcf_dir, file)\n",
        "        aggregated_df = process_vcf(file_path)\n",
        "\n",
        "        # Save each processed file as a CSV\n",
        "        output_file = os.path.join(output_dir, file.replace('.vcf.gz', '_aggregated.csv'))\n",
        "        aggregated_df.to_csv(output_file, index=False)\n",
        "        print(f\"Saved aggregated data to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "6XIKIkzxefIp",
        "outputId": "dba37191-8647-4a34-93d0-cc3540c3ba7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vcfpy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-58fb009fa3c9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvcfpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the directory where your .vcf.gz files are stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vcfpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vcfpy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the panel file (assuming it contains columns 'sample', 'pop' for population and 'gender')\n",
        "panel_file_path = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/integrated_call_samples_v3.20130502.ALL.panel.txt'  # Replace with actual path\n",
        "panel_df = pd.read_csv(panel_file_path, sep='\\t')\n",
        "\n",
        "# Function to clean the VCF file and include location name and gender\n",
        "def clean_vcf_with_location(file_path, panel_df, limit=10):\n",
        "    reader = vcfpy.Reader.from_path(file_path)\n",
        "\n",
        "    records = []\n",
        "    count = 0\n",
        "\n",
        "    for record in reader:\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "        chrom = record.CHROM\n",
        "        pos = record.POS\n",
        "        ref = record.REF\n",
        "\n",
        "        # Simplify ALT field (keep only the value, remove type information)\n",
        "        alt = ','.join([str(a.value) for a in record.ALT])\n",
        "\n",
        "        # Extract relevant fields from the INFO dictionary\n",
        "        allele_count = record.INFO.get('AC', ['Unknown'])[0]\n",
        "        allele_freq = record.INFO.get('AF', ['Unknown'])[0]\n",
        "\n",
        "        # Extract the sample IDs from the calls and map them to the location name and gender\n",
        "        for call in record.calls:\n",
        "            sample_id = call.sample\n",
        "            sample_info = panel_df[panel_df['sample'] == sample_id]\n",
        "\n",
        "            if not sample_info.empty:\n",
        "                population_code = sample_info['pop'].values[0]\n",
        "                gender = sample_info['gender'].values[0]  # Assuming 'gender' is a column in the panel file\n",
        "\n",
        "                # Map population code to country name\n",
        "                location_name = country_code_to_name.get(population_code, 'Unknown')\n",
        "\n",
        "                # Append the relevant data\n",
        "                records.append([chrom, pos, ref, alt, allele_count, allele_freq, location_name, gender])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Convert to a DataFrame for better viewing and manipulation\n",
        "    df = pd.DataFrame(records, columns=['CHROM', 'POS', 'REF', 'ALT', 'Allele_Count', 'Allele_Frequency', 'Location_Name', 'Gender'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# View the cleaned data for the first 10 records\n",
        "vcf_file_path = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz'  # Replace with your actual VCF file path\n",
        "cleaned_vcf_data = clean_vcf_with_location(vcf_file_path, panel_df, limit=10)\n",
        "print(cleaned_vcf_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8lIxo0NZlY9",
        "outputId": "2dc03eff-d61a-414e-c3de-b5d8ebaf898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      CHROM    POS                     REF ALT  Allele_Count  \\\n",
            "0         1  10177                       A  AC          2130   \n",
            "1         1  10177                       A  AC          2130   \n",
            "2         1  10177                       A  AC          2130   \n",
            "3         1  10177                       A  AC          2130   \n",
            "4         1  10177                       A  AC          2130   \n",
            "...     ...    ...                     ...  ..           ...   \n",
            "25035     1  10616  CCGCCGTTGCAAAGGCGCGCCG   C          4973   \n",
            "25036     1  10616  CCGCCGTTGCAAAGGCGCGCCG   C          4973   \n",
            "25037     1  10616  CCGCCGTTGCAAAGGCGCGCCG   C          4973   \n",
            "25038     1  10616  CCGCCGTTGCAAAGGCGCGCCG   C          4973   \n",
            "25039     1  10616  CCGCCGTTGCAAAGGCGCGCCG   C          4973   \n",
            "\n",
            "       Allele_Frequency   Location_Name  Gender  \n",
            "0              0.425319  United Kingdom    male  \n",
            "1              0.425319  United Kingdom  female  \n",
            "2              0.425319  United Kingdom  female  \n",
            "3              0.425319  United Kingdom  female  \n",
            "4              0.425319  United Kingdom    male  \n",
            "...                 ...             ...     ...  \n",
            "25035          0.993011           India  female  \n",
            "25036          0.993011           India  female  \n",
            "25037          0.993011           India  female  \n",
            "25038          0.993011           India  female  \n",
            "25039          0.993011           India  female  \n",
            "\n",
            "[25040 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming cleaned_vcf_data is your dataframe with relevant columns\n",
        "# Group by Gender and Location_Name\n",
        "grouped_data = cleaned_vcf_data.groupby(['Gender', 'Location_Name']).agg({\n",
        "    'Allele_Frequency': 'mean',  # Calculate mean allele frequency\n",
        "    'Allele_Count': 'sum'        # Sum allele count (or any other relevant aggregation)\n",
        "}).reset_index()\n",
        "\n",
        "# Display the grouped data\n",
        "print(grouped_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qqWL0SSk545",
        "outputId": "04bbfb14-2d55-41bf-f81c-c581dff0e2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Gender     Location_Name  Allele_Frequency  Allele_Count\n",
            "0   female  African American          0.185863        325780\n",
            "1   female        Bangladesh          0.185863        409552\n",
            "2   female         Caribbean          0.185863        456092\n",
            "3   female             China          0.185863       1479972\n",
            "4   female          Colombia          0.185863        474708\n",
            "5   female           Finland          0.185863        567788\n",
            "6   female            Gambia          0.185863        539864\n",
            "7   female             India          0.185863        837720\n",
            "8   female             Italy          0.185863        502632\n",
            "9   female             Japan          0.185863        446784\n",
            "10  female             Kenya          0.185863        511940\n",
            "11  female            Mexico          0.185863        297856\n",
            "12  female           Nigeria          0.185863        949416\n",
            "13  female          Pakistan          0.185863        446784\n",
            "14  female              Peru          0.185863        409552\n",
            "15  female       Puerto Rico          0.185863        465400\n",
            "16  female      Sierra Leone          0.185863        400244\n",
            "17  female             Spain          0.185863        493324\n",
            "18  female         Sri Lanka          0.185863        437476\n",
            "19  female    United Kingdom          0.185863        418860\n",
            "20  female     United States          0.185863        465400\n",
            "21  female           Vietnam          0.185863        493324\n",
            "22    male  African American          0.185863        242008\n",
            "23    male        Bangladesh          0.185863        390936\n",
            "24    male         Caribbean          0.185863        437476\n",
            "25    male             China          0.185863       1321736\n",
            "26    male          Colombia          0.185863        400244\n",
            "27    male           Finland          0.185863        353704\n",
            "28    male            Gambia          0.185863        511940\n",
            "29    male             India          0.185863       1070420\n",
            "30    male             Italy          0.185863        493324\n",
            "31    male             Japan          0.185863        521248\n",
            "32    male             Kenya          0.185863        409552\n",
            "33    male            Mexico          0.185863        297856\n",
            "34    male           Nigeria          0.185863        977340\n",
            "35    male          Pakistan          0.185863        446784\n",
            "36    male              Peru          0.185863        381628\n",
            "37    male       Puerto Rico          0.185863        502632\n",
            "38    male      Sierra Leone          0.185863        390936\n",
            "39    male             Spain          0.185863        502632\n",
            "40    male         Sri Lanka          0.185863        511940\n",
            "41    male    United Kingdom          0.185863        428168\n",
            "42    male     United States          0.185863        456092\n",
            "43    male           Vietnam          0.185863        428168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vcfpy\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean the VCF file (excluding the samples list)\n",
        "def clean_vcf(file_path, limit=10):\n",
        "    reader = vcfpy.Reader.from_path(file_path)\n",
        "\n",
        "    records = []\n",
        "    count = 0\n",
        "\n",
        "    for record in reader:\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "        chrom = record.CHROM\n",
        "        pos = record.POS\n",
        "        ref = record.REF\n",
        "\n",
        "        # Simplify ALT field (keep only the value, remove type information)\n",
        "        alt = ','.join([str(a.value) for a in record.ALT])\n",
        "\n",
        "        # Extract relevant fields from the INFO dictionary\n",
        "        allele_count = record.INFO.get('AC', ['Unknown'])[0]\n",
        "        allele_freq = record.INFO.get('AF', ['Unknown'])[0]\n",
        "\n",
        "        # Append the necessary fields to the records list\n",
        "        records.append([chrom, pos, ref, alt, allele_count, allele_freq])\n",
        "        count += 1\n",
        "\n",
        "    # Convert to a DataFrame for better viewing and manipulation\n",
        "    df = pd.DataFrame(records, columns=['CHROM', 'POS', 'REF', 'ALT', 'Allele_Count', 'Allele_Frequency'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# View the cleaned data for the first 10 records\n",
        "vcf_file_path = vcf_file_path = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz'  # Replace with your actual VCF file path\n",
        "cleaned_vcf_data = clean_vcf(vcf_file_path, limit=10)\n",
        "print(cleaned_vcf_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SOCLkkDaTei",
        "outputId": "09cb2919-c7c2-48fa-e252-d3cfba15581e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CHROM    POS                     REF ALT  Allele_Count  Allele_Frequency\n",
            "0     1  10177                       A  AC          2130          0.425319\n",
            "1     1  10235                       T  TA             6          0.001198\n",
            "2     1  10352                       T  TA          2191          0.437500\n",
            "3     1  10505                       A   T             1          0.000200\n",
            "4     1  10506                       C   G             1          0.000200\n",
            "5     1  10511                       G   A             1          0.000200\n",
            "6     1  10539                       C   A             3          0.000599\n",
            "7     1  10542                       C   T             1          0.000200\n",
            "8     1  10579                       C   A             1          0.000200\n",
            "9     1  10616  CCGCCGTTGCAAAGGCGCGCCG   C          4973          0.993011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the population metadata file\n",
        "panel_file_path = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/integrated_call_samples_v3.20130502.ALL.panel.txt'\n",
        "panel_df = pd.read_csv(panel_file_path, sep='\\t')\n",
        "\n",
        "# View the first few rows of the panel file to understand its structure\n",
        "print(panel_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA_zc-X1b_88",
        "outputId": "4aae123a-6ee2-43ec-bba6-0708252672ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    sample  pop super_pop  gender  Unnamed: 4  Unnamed: 5\n",
            "0  HG00096  GBR       EUR    male         NaN         NaN\n",
            "1  HG00097  GBR       EUR  female         NaN         NaN\n",
            "2  HG00099  GBR       EUR  female         NaN         NaN\n",
            "3  HG00100  GBR       EUR  female         NaN         NaN\n",
            "4  HG00101  GBR       EUR    male         NaN         NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the panel_df for a specific population (e.g., GBR for United Kingdom)\n",
        "gbr_samples = panel_df[panel_df['pop'] == 'GBR']['sample'].tolist()\n",
        "\n",
        "# Now you can use this list of 'gbr_samples' to filter your VCF data for just UK samples\n",
        "import pandas as pd\n",
        "\n",
        "# Read the metadata file\n",
        "metadata_file_path = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/integrated_call_samples_v3.20130502.ALL.panel.txt'\n",
        "metadata = pd.read_csv(metadata_file_path, sep='\\t')\n",
        "metadata_cleaned = metadata.drop(columns=['Unnamed: 4', 'Unnamed: 5'])\n",
        "\n",
        "# Display the cleaned metadata\n",
        "print(metadata_cleaned.head())\n",
        "\n",
        "# Check for any remaining issues in the metadata\n",
        "print(metadata_cleaned.info())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYooRjvJMyug",
        "outputId": "b4830c53-88e9-4b7a-d2b6-960506ff7733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    sample  pop super_pop  gender\n",
            "0  HG00096  GBR       EUR    male\n",
            "1  HG00097  GBR       EUR  female\n",
            "2  HG00099  GBR       EUR  female\n",
            "3  HG00100  GBR       EUR  female\n",
            "4  HG00101  GBR       EUR    male\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2504 entries, 0 to 2503\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   sample     2504 non-null   object\n",
            " 1   pop        2504 non-null   object\n",
            " 2   super_pop  2504 non-null   object\n",
            " 3   gender     2504 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 78.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vcfpy\n",
        "# Path to the VCF file\n",
        "vcf_file_path = '/content/drive/My Drive/AWS_Redshift/Genomics_Data/ALL.chr20.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz'\n",
        "\n",
        "# Open the VCF file using vcfpy\n",
        "reader = vcfpy.Reader.from_path(vcf_file_path)\n",
        "\n",
        "# Prepare to collect data\n",
        "records = []\n",
        "\n",
        "# Iterate over records in the VCF file\n",
        "for i, record in enumerate(reader):\n",
        "    row = {\n",
        "        'CHROM': record.CHROM,\n",
        "        'POS': record.POS,\n",
        "        'ID': record.ID,\n",
        "        'REF': record.REF,\n",
        "        'ALT': str(record.ALT[0])\n",
        "    }\n",
        "    # Add genotype information for each sample\n",
        "    for call in record.calls:\n",
        "        sample_name = call.sample\n",
        "        genotype = call.data.get('GT', './.')  # Default to './.' if missing\n",
        "        row[sample_name] = genotype\n",
        "    records.append(row)\n",
        "\n",
        "    if i >= 5:  # Limit for testing\n",
        "        break\n",
        "\n",
        "# Convert to DataFrame\n",
        "variants_df = pd.DataFrame(records)\n",
        "\n",
        "# Display the first few rows\n",
        "print(variants_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCKcDf7T9gC8",
        "outputId": "065ce70b-699a-497b-dc70-24ab62c8f1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CHROM    POS  ID REF                                    ALT HG00096 HG00097  \\\n",
            "0    20  60343  []   G   Substitution(type_='SNV', value='A')     0|0     0|0   \n",
            "1    20  60419  []   A   Substitution(type_='SNV', value='G')     0|0     0|0   \n",
            "2    20  60479  []   C   Substitution(type_='SNV', value='T')     0|0     0|0   \n",
            "3    20  60522  []   T  Substitution(type_='INS', value='TC')     0|0     0|0   \n",
            "4    20  60568  []   A   Substitution(type_='SNV', value='C')     0|0     0|0   \n",
            "\n",
            "  HG00099 HG00100 HG00101  ... NA21128 NA21129 NA21130 NA21133 NA21135  \\\n",
            "0     0|0     0|0     0|0  ...     0|0     0|0     0|0     0|0     0|0   \n",
            "1     0|0     0|0     0|0  ...     0|0     0|0     0|0     0|0     0|0   \n",
            "2     0|0     0|0     0|0  ...     0|0     0|0     0|0     0|0     0|0   \n",
            "3     0|0     0|0     0|0  ...     0|0     0|0     0|0     0|0     0|0   \n",
            "4     0|0     0|0     0|0  ...     0|0     0|0     0|0     0|0     0|0   \n",
            "\n",
            "  NA21137 NA21141 NA21142 NA21143 NA21144  \n",
            "0     0|0     0|0     0|0     0|0     0|0  \n",
            "1     0|0     0|0     0|0     0|0     0|0  \n",
            "2     0|0     0|0     0|0     0|0     0|0  \n",
            "3     0|0     0|0     0|0     0|0     0|0  \n",
            "4     0|0     0|0     0|0     0|0     0|0  \n",
            "\n",
            "[5 rows x 2509 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sample IDs from VCF DataFrame (assuming you have variants_df from the VCF)\n",
        "vcf_sample_ids = variants_df.columns[5:]  # Adjust this if your DataFrame structure is different\n",
        "\n",
        "# Verify matching sample IDs\n",
        "matching_samples = metadata_cleaned['sample'].isin(vcf_sample_ids)\n",
        "print(metadata_cleaned[matching_samples].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXRChjQf2Drt",
        "outputId": "dcdb0d1d-fc4c-46cb-a01a-5bfd8933c46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    sample  pop super_pop  gender\n",
            "0  HG00096  GBR       EUR    male\n",
            "1  HG00097  GBR       EUR  female\n",
            "2  HG00099  GBR       EUR  female\n",
            "3  HG00100  GBR       EUR  female\n",
            "4  HG00101  GBR       EUR    male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape variants_df if necessary to long format for merging (depending on your analysis)\n",
        "# Example: pivot or melt to align with metadata structure\n",
        "\n",
        "# Merge datasets\n",
        "combined_df = pd.melt(variants_df, id_vars=['CHROM', 'POS', 'ID', 'REF', 'ALT'],\n",
        "                      var_name='sample', value_name='genotype')\n",
        "\n",
        "# Merge with metadata\n",
        "integrated_data = combined_df.merge(metadata_cleaned, on='sample')\n",
        "\n",
        "# Display the integrated dataset\n",
        "print(integrated_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rms0AXzw2PWU",
        "outputId": "de2b5009-1c7b-4d38-c422-9a44d5fa64e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      CHROM    POS  ID REF                                    ALT   sample  \\\n",
            "0        20  60343  []   G   Substitution(type_='SNV', value='A')  HG00096   \n",
            "1        20  60419  []   A   Substitution(type_='SNV', value='G')  HG00096   \n",
            "2        20  60479  []   C   Substitution(type_='SNV', value='T')  HG00096   \n",
            "3        20  60522  []   T  Substitution(type_='INS', value='TC')  HG00096   \n",
            "4        20  60568  []   A   Substitution(type_='SNV', value='C')  HG00096   \n",
            "...     ...    ...  ..  ..                                    ...      ...   \n",
            "15019    20  60419  []   A   Substitution(type_='SNV', value='G')  NA21144   \n",
            "15020    20  60479  []   C   Substitution(type_='SNV', value='T')  NA21144   \n",
            "15021    20  60522  []   T  Substitution(type_='INS', value='TC')  NA21144   \n",
            "15022    20  60568  []   A   Substitution(type_='SNV', value='C')  NA21144   \n",
            "15023    20  60571  []   C   Substitution(type_='SNV', value='A')  NA21144   \n",
            "\n",
            "      genotype  pop super_pop  gender  \n",
            "0          0|0  GBR       EUR    male  \n",
            "1          0|0  GBR       EUR    male  \n",
            "2          0|0  GBR       EUR    male  \n",
            "3          0|0  GBR       EUR    male  \n",
            "4          0|0  GBR       EUR    male  \n",
            "...        ...  ...       ...     ...  \n",
            "15019      0|0  GIH       SAS  female  \n",
            "15020      0|0  GIH       SAS  female  \n",
            "15021      0|0  GIH       SAS  female  \n",
            "15022      0|0  GIH       SAS  female  \n",
            "15023      0|0  GIH       SAS  female  \n",
            "\n",
            "[15024 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_CALCIUM_Y2024M06D05.CSV'\n",
        "calcium_actual = pd.read_csv(file_path)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = calcium_actual.isnull().sum()\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicate_rows = calcium_actual.duplicated().sum()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['measure_id', 'measure_name', 'location_set', 'metric_id', 'metric_name']\n",
        "calcium_actual_cleaned = calcium_actual.drop(columns=columns_to_drop)\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(calcium_actual_cleaned.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2o9Su2qwG1S",
        "outputId": "89eb8107-dc44-415d-bd99-682e9827c7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   location_id location_name  sex_id sex_name  age_group_id age_group_name  \\\n",
            "0            6         China       3     Both            10       25 to 29   \n",
            "1            6         China       3     Both            11       30 to 34   \n",
            "2            6         China       3     Both            12       35 to 39   \n",
            "3            6         China       3     Both            13       40 to 44   \n",
            "4            6         China       3     Both            14       45 to 49   \n",
            "\n",
            "   year_id   unit       val     upper     lower  \n",
            "0     1990  g/day  0.320990  0.337980  0.302906  \n",
            "1     1990  g/day  0.313640  0.332607  0.295435  \n",
            "2     1990  g/day  0.327698  0.347042  0.309903  \n",
            "3     1990  g/day  0.336522  0.356920  0.317888  \n",
            "4     1990  g/day  0.342058  0.362077  0.322839  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading each dataset\n",
        "import pandas as pd\n",
        "\n",
        "def load_and_clean(file_path, value_column_name):\n",
        "    \"\"\"\n",
        "    Load and clean a CSV file, returning a cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = ['measure_id', 'measure_name', 'location_set', 'metric_id', 'metric_name',\n",
        "                       'upper', 'lower', 'unit', 'age_group_id', 'sex_id', 'location_id']\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Rename the 'val' column to a specific value column name for clarity\n",
        "    df.rename(columns={'val': value_column_name}, inplace=True)\n",
        "\n",
        "    # Drop duplicates\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Standardize column names\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # Filter for relevant data if needed, for example specific years\n",
        "    df_filtered = df[(df['year_id'] >= 2000) & (df['year_id'] <= 2020)]\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "# Example usage\n",
        "# Define the file paths and corresponding column names\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file paths and corresponding value column names\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_CALCIUM_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_FIBER_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_FRUIT_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_LEGUMES_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_MILK_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_NUTS_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_OMEGA3_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_PROCMEAT_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_PUFA_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_REDMEAT_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_SODIUM_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_SSBS_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_TRANSFAT_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_VEG_Y2024M06D05.CSV',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/IHME_GBD_2021_DIET_RISK_1990_2021/IHME_GBD_2021_DIET_RISK_1990_2021_WHOLEGRAINS_Y2024M06D05.CSV'\n",
        "]\n",
        "\n",
        "\n",
        "value_column_names = [\n",
        "    'calcium_intake_g_per_day',\n",
        "    'fiber_intake_g_per_day',\n",
        "    'fruit_intake_g_per_day',\n",
        "    'legumes_intake_g_per_day',\n",
        "    'milk_intake_g_per_day',\n",
        "    'nuts_intake_g_per_day',\n",
        "    'omega3_intake_g_per_day',\n",
        "    'processed_meat_intake_g_per_day',\n",
        "    'pufa_intake_g_per_day',\n",
        "    'red_meat_intake_g_per_day',\n",
        "    'sodium_intake_g_per_day',\n",
        "    'ssbs_intake_g_per_day',  # ssbs: Sugar-Sweetened Beverages\n",
        "    'transfat_intake_g_per_day',\n",
        "    'veg_intake_g_per_day',\n",
        "    'wholegrains_intake_g_per_day',\n",
        "]\n",
        "\n",
        "# Load and clean each dataset\n",
        "cleaned_dataframes = [load_and_clean(file_path, value_column_name)\n",
        "                      for file_path, value_column_name in zip(file_paths, value_column_names)]\n",
        "\n",
        "#2.Aggregate and Merge\n",
        "# Merge all cleaned datasets on common columns such as 'location_name', 'age_group_name', 'year_id'\n",
        "merged_data = cleaned_dataframes[0]\n",
        "for df in cleaned_dataframes[1:]:\n",
        "    merged_data = pd.merge(merged_data, df, on=['location_name', 'age_group_name', 'year_id', 'sex_name'], how='outer')\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(merged_data.head())\n",
        "\n",
        "# Save the merged DataFrame to a new CSV file\n",
        "merged_data.to_csv('/content/drive/MyDrive/AWS_Redshift/merged_lifestyle_data.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1uXyAtCVG1h",
        "outputId": "1ea04ee9-a404-412c-fece-2c536f375afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  location_name sex_name age_group_name  year_id  calcium_intake_g_per_day  \\\n",
            "0         China     Both       25 to 29     2000                  0.363979   \n",
            "1         China     Both       30 to 34     2000                  0.351419   \n",
            "2         China     Both       35 to 39     2000                  0.370753   \n",
            "3         China     Both       40 to 44     2000                  0.382709   \n",
            "4         China     Both       45 to 49     2000                  0.389289   \n",
            "\n",
            "   fiber_intake_g_per_day  fruit_intake_g_per_day  legumes_intake_g_per_day  \\\n",
            "0               13.013235               65.477688                 29.620451   \n",
            "1               12.361805               62.253029                 30.465697   \n",
            "2               13.194327               66.207218                 31.423916   \n",
            "3               13.489024               85.938181                 31.918052   \n",
            "4               13.940915               84.833026                 32.168174   \n",
            "\n",
            "   milk_intake_g_per_day  nuts_intake_g_per_day  omega3_intake_g_per_day  \\\n",
            "0              16.181662               3.729136                 0.226834   \n",
            "1             147.631385               3.997413                 0.228494   \n",
            "2              15.401402               4.223271                 0.231218   \n",
            "3              14.504607               4.475803                 0.238590   \n",
            "4              14.761950               4.735628                 0.249111   \n",
            "\n",
            "   processed_meat_intake_g_per_day  pufa_intake_g_per_day  \\\n",
            "0                         2.818826               1.200590   \n",
            "1                         2.794532               1.212488   \n",
            "2                         2.756746               1.234870   \n",
            "3                         2.692723               1.259717   \n",
            "4                         2.599587               1.268461   \n",
            "\n",
            "   red_meat_intake_g_per_day  sodium_intake_g_per_day  ssbs_intake_g_per_day  \\\n",
            "0                  34.319327                 7.124995              15.733011   \n",
            "1                  34.344940                 7.329786              13.167664   \n",
            "2                  34.236586                 7.627251              10.699314   \n",
            "3                  33.743263                 7.631220               8.919336   \n",
            "4                  32.628284                 7.643623               8.087337   \n",
            "\n",
            "   transfat_intake_g_per_day  veg_intake_g_per_day  \\\n",
            "0                   0.474479            256.469986   \n",
            "1                   0.478304            295.585032   \n",
            "2                   0.481786            295.893063   \n",
            "3                   0.487125            269.841359   \n",
            "4                   0.498882            276.905310   \n",
            "\n",
            "   wholegrains_intake_g_per_day  \n",
            "0                     17.763585  \n",
            "1                     19.460095  \n",
            "2                     21.145723  \n",
            "3                     22.338471  \n",
            "4                     22.796702  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the merged lifestyle data\n",
        "lifestyle_data_path = '/content/drive/MyDrive/AWS_Redshift/merged_lifestyle_data.csv'\n",
        "merged_lifestyle_data = pd.read_csv(lifestyle_data_path)\n",
        "\n",
        "# Map the population codes to full country names\n",
        "mapped_countries = [country_code_to_name[code] for code in unique_countries]\n",
        "\n",
        "# Normalize country names in both datasets (lowercase and strip spaces)\n",
        "merged_lifestyle_data['location_name'] = merged_lifestyle_data['location_name'].str.lower().str.strip()\n",
        "mapped_countries = [name.lower().strip() for name in mapped_countries]\n",
        "\n",
        "# Filter the lifestyle data to only include countries present in the genomics data (mapped names)\n",
        "filtered_lifestyle_data = merged_lifestyle_data[merged_lifestyle_data['location_name'].isin(mapped_countries)]\n",
        "\n",
        "# Remove rows where sex_name is \"Both\"\n",
        "filtered_lifestyle_data = filtered_lifestyle_data[filtered_lifestyle_data['sex_name'] != 'Both']\n",
        "\n",
        "# Remove the 'age_group_name' column explicitly\n",
        "filtered_lifestyle_data = filtered_lifestyle_data.drop(columns=['age_group_name'])\n",
        "\n",
        "# Average values across all other columns (group by location_name, sex_name, and year_id)\n",
        "filtered_lifestyle_data = filtered_lifestyle_data.groupby(\n",
        "    ['location_name', 'sex_name', 'year_id'], as_index=False\n",
        ").mean()\n",
        "\n",
        "# Save the filtered data to a new CSV file\n",
        "filtered_lifestyle_data.to_csv('filtered_nutrition_data.csv', index=False)\n",
        "\n",
        "# Save the filtered data to Google Drive with the name filtered_nutrition_data.csv\n",
        "filtered_lifestyle_data.to_csv('/content/drive/My Drive/AWS_Redshift/filtered_nutrition_data_v2.csv', index=False)\n",
        "\n",
        "print(f\"Filtered lifestyle data contains {filtered_lifestyle_data.shape[0]} rows.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nR8AYXFsOl9",
        "outputId": "0d92be1d-0511-42d6-fb9b-b590ebabc9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered lifestyle data contains 798 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering Tobacco data to only include countries present in the genomics dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the tobacco usage file\n",
        "tobacco_data_path = '/content/drive/MyDrive/AWS_Redshift/Tobacco_usage_cleaned.xlsx'  # Replace with your actual file path\n",
        "tobacco_data = pd.read_excel(tobacco_data_path)\n",
        "\n",
        "# Convert the dictionary values (country names) into a list\n",
        "genomics_countries_list = list(country_code_to_name.values())\n",
        "\n",
        "# Filter the tobacco data to only include countries present in genomics data\n",
        "filtered_tobacco_data = tobacco_data[tobacco_data['location_name'].isin(genomics_countries_list)]\n",
        "\n",
        "# Save the filtered data to a new CSV file\n",
        "filtered_tobacco_data.to_csv('filtered_tobacco_data.csv', index=False)\n",
        "filtered_tobacco_data.to_csv('/content/drive/My Drive/AWS_Redshift/filtered_tobacco_data.csv', index=False)\n",
        "\n",
        "print(f\"Filtered tobacco data contains {filtered_tobacco_data.shape[0]} rows.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X72yNxa5ujf",
        "outputId": "df6b2e09-2f6b-444f-d1c2-5a98502ac271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tobacco data contains 274 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001400.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Group by 'TimeDim', 'ParentLocation', and 'DisaggregatingDimension1ValueCode', then calculate the mean of 'NumericValue'\n",
        "df_grouped = df.groupby(['TimeDim', 'ParentLocation', 'DisaggregatingDimension1ValueCode'])['NumericValue'].mean().reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "df_grouped.rename(columns={\n",
        "    'ParentLocation': 'parent_location',\n",
        "    'TimeDim': 'year',\n",
        "    'DisaggregatingDimension1ValueCode': 'alcohol_type',\n",
        "    'NumericValue': 'average_numeric_value'\n",
        "}, inplace=True)\n",
        "\n",
        "# Print the grouped and averaged DataFrame\n",
        "print(\"\\nGrouped and Averaged Data Overview:\")\n",
        "print(df_grouped.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nygyjHB3KQBq",
        "outputId": "974988a0-c89d-4506-9e0d-459706b24a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grouped and Averaged Data Overview:\n",
            "   year parent_location            alcohol_type  average_numeric_value\n",
            "0  1960        Americas     ALCOHOLTYPE_SA_BEER                   3.75\n",
            "1  1960        Americas  ALCOHOLTYPE_SA_SPIRITS                   3.26\n",
            "2  1960        Americas    ALCOHOLTYPE_SA_TOTAL                   7.83\n",
            "3  1960        Americas     ALCOHOLTYPE_SA_WINE                   0.83\n",
            "4  1960          Europe     ALCOHOLTYPE_SA_BEER                   4.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Filter for years between 2000 and 2020\n",
        "    df = df[(df['TimeDim'] >= 2000) & (df['TimeDim'] <= 2020)]\n",
        "\n",
        "    # Group by 'TimeDim', 'ParentLocation', and 'DisaggregatingDimension1ValueCode', then calculate the mean of 'NumericValue'\n",
        "    df_grouped = df.groupby(['TimeDim', 'ParentLocation', 'DisaggregatingDimension1ValueCode'])['NumericValue'].mean().reset_index()\n",
        "\n",
        "    # Rename columns for clarity\n",
        "    df_grouped.rename(columns={\n",
        "        'ParentLocation': 'parent_location',\n",
        "        'TimeDim': 'year',\n",
        "        'DisaggregatingDimension1ValueCode': 'alcohol_type',\n",
        "        'NumericValue': 'average_numeric_value'\n",
        "    }, inplace=True)\n",
        "\n",
        "    return df_grouped\n",
        "\n",
        "# Example file paths\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001400.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001404.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001688.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001747.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001751.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001818.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001821.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001823.csv'\n",
        "\n",
        "]\n"
      ],
      "metadata": {
        "id": "ehcZw43Rjmyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dataframes(file_paths):\n",
        "    combined_df = pd.DataFrame()  # Initialize an empty DataFrame for combined results\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        current_df = process_file(file_path)\n",
        "\n",
        "        # Merge strategy to add only new countries' data\n",
        "        if combined_df.empty:\n",
        "            combined_df = current_df\n",
        "        else:\n",
        "            # Merge while ensuring no duplicate entries for countries already included\n",
        "            combined_df = pd.merge(combined_df, current_df, on=['year', 'parent_location', 'alcohol_type'], how='outer', suffixes=('', '_dup'))\n",
        "\n",
        "            # Handle duplicates: keep the original if duplicated, otherwise take from the new file\n",
        "            for col in combined_df.columns:\n",
        "                if '_dup' in col:\n",
        "                    combined_df[col.replace('_dup', '')].fillna(combined_df[col], inplace=True)\n",
        "                    combined_df.drop(columns=col, inplace=True)\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "# Process all files and merge\n",
        "final_data = merge_dataframes(file_paths)\n",
        "print(final_data.head())\n",
        "final_data.to_csv('/content/drive/MyDrive/AWS_Redshift/Alc_levels/combined_alcohol_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB2gTa87kSvi",
        "outputId": "27c5adff-326e-4fd3-e101-a16c0c0e06e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year parent_location                  alcohol_type  average_numeric_value\n",
            "0  2000          Africa           ALCOHOLTYPE_SA_BEER               0.983990\n",
            "1  2000          Africa  ALCOHOLTYPE_SA_OTHER_ALCOHOL               1.515731\n",
            "2  2000          Africa        ALCOHOLTYPE_SA_SPIRITS               0.545006\n",
            "3  2000          Africa          ALCOHOLTYPE_SA_TOTAL               3.407420\n",
            "4  2000          Africa           ALCOHOLTYPE_SA_WINE               0.362693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/AWS_Redshift/Alc_levels/mapped_combined_alcohol_data.csv'  # Adjust path if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 3: Explode the 'mapped_country' column into separate rows\n",
        "import ast\n",
        "\n",
        "# Convert the 'mapped_country' from string representation to a list\n",
        "data['mapped_country'] = data['mapped_country'].apply(ast.literal_eval)  # If stored as a string list\n",
        "\n",
        "# Explode the 'mapped_country' to have one row per country\n",
        "data_exploded = data.explode('mapped_country')\n",
        "\n",
        "# Rename 'mapped_country' to 'location_name' and 'year' to 'year_id'\n",
        "data_exploded.rename(columns={'mapped_country': 'location_name', 'year': 'year_id'}, inplace=True)\n",
        "\n",
        "# Step 4: Calculate the total value of 'average_numeric_value' per location_name and year_id\n",
        "# Group by 'location_name' and 'year_id' and sum 'average_numeric_value'\n",
        "aggregated_data = data_exploded.groupby(['location_name', 'year_id']).agg(\n",
        "    total_average_value=('average_numeric_value', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Step 5: Create the 'sex_name' column with 70% for Male and 30% for Female\n",
        "def split_gender_values(row):\n",
        "    male_value = row['total_average_value'] * 0.7\n",
        "    female_value = row['total_average_value'] * 0.3\n",
        "    return pd.DataFrame({\n",
        "        'location_name': [row['location_name'], row['location_name']],\n",
        "        'year_id': [row['year_id'], row['year_id']],\n",
        "        'sex_name': ['Male', 'Female'],\n",
        "        'gender_specific_value': [male_value, female_value]\n",
        "    })\n",
        "\n",
        "# Apply the gender split function to the aggregated data\n",
        "gender_split_data = aggregated_data.apply(split_gender_values, axis=1)\n",
        "\n",
        "# Concatenate the resulting DataFrames\n",
        "final_data = pd.concat(gender_split_data.tolist(), ignore_index=True)\n",
        "\n",
        "# Step 6: Save the final result back to Google Drive as a new CSV file\n",
        "output_file_path = '/content/drive/MyDrive/AWS_Redshift/Alc_levels/mapped_combined_alcohol_data_v2.csv'\n",
        "final_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Print a message indicating the file was saved successfully\n",
        "print(f\"Final gender-split data has been saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTqd8Y-oKQUN",
        "outputId": "17ba7fbb-1d0d-479c-e98b-347f301176b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final gender-split data has been saved to: /content/drive/MyDrive/AWS_Redshift/Alc_levels/mapped_combined_alcohol_data_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKING IF IT'S PROPERLY MERGED\n",
        "import pandas as pd\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df[(df['TimeDim'] >= 2000) & (df['TimeDim'] <= 2020)]  # Filter years\n",
        "    grouped_df = df.groupby(['TimeDim', 'ParentLocation', 'DisaggregatingDimension1ValueCode'])['NumericValue'].mean().reset_index()\n",
        "    grouped_df.rename(columns={\n",
        "        'ParentLocation': 'parent_location',\n",
        "        'TimeDim': 'year',\n",
        "        'DisaggregatingDimension1ValueCode': 'alcohol_type',\n",
        "        'NumericValue': 'average_numeric_value'\n",
        "    }, inplace=True)\n",
        "\n",
        "    print(f\"Processed {file_path}: {len(grouped_df['parent_location'].unique())} countries, {len(grouped_df['year'].unique())} years\")\n",
        "    return grouped_df\n",
        "def merge_dataframes(file_paths):\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        current_df = process_file(file_path)\n",
        "        if combined_df.empty:\n",
        "            combined_df = current_df\n",
        "        else:\n",
        "            combined_df = pd.merge(combined_df, current_df, on=['year', 'parent_location', 'alcohol_type'], how='outer', suffixes=('', '_dup'))\n",
        "\n",
        "            # Resolve any duplicates, prioritizing non-duplicated values\n",
        "            for col in combined_df.columns:\n",
        "                if '_dup' in col:\n",
        "                    combined_df[col.replace('_dup', '')].fillna(combined_df[col], inplace=True)\n",
        "                    combined_df.drop(columns=col, inplace=True)\n",
        "\n",
        "        print(f\"After merging {file_path}: {combined_df.shape[0]} records\")\n",
        "\n",
        "    return combined_df\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001400.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001404.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001688.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001747.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001751.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001818.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001821.csv',\n",
        "    '/content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001823.csv'\n",
        "\n",
        "]\n",
        "\n",
        "# Process and merge all files\n",
        "final_data = merge_dataframes(file_paths)\n",
        "\n",
        "# Final integrity checks\n",
        "print(\"Final merged data overview:\")\n",
        "print(final_data.info())\n",
        "print(\"Unique years in final dataset:\", final_data['year'].unique())\n",
        "print(\"Unique countries in final dataset:\", final_data['parent_location'].unique())\n",
        "print(\"Any missing values:\", final_data.isnull().any())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4UH8Fckkhg",
        "outputId": "b0727dc7-82d2-4b8e-cd06-4cc07ed51acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001400.csv: 6 countries, 21 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001400.csv: 630 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001404.csv: 6 countries, 21 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001404.csv: 1008 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001688.csv: 6 countries, 21 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001688.csv: 1008 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001747.csv: 0 countries, 0 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001747.csv: 1008 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001751.csv: 6 countries, 21 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001751.csv: 1008 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001818.csv: 0 countries, 0 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001818.csv: 1008 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001821.csv: 0 countries, 0 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001821.csv: 1008 records\n",
            "Processed /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001823.csv: 0 countries, 0 years\n",
            "After merging /content/drive/MyDrive/AWS_Redshift/Alc_levels/SA_0000001823.csv: 1008 records\n",
            "Final merged data overview:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1008 entries, 0 to 1007\n",
            "Data columns (total 4 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   year                   1008 non-null   int64  \n",
            " 1   parent_location        1008 non-null   object \n",
            " 2   alcohol_type           1008 non-null   object \n",
            " 3   average_numeric_value  1008 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(2)\n",
            "memory usage: 31.6+ KB\n",
            "None\n",
            "Unique years in final dataset: [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n",
            " 2014 2015 2016 2017 2018 2019 2020]\n",
            "Unique countries in final dataset: ['Africa' 'Americas' 'Eastern Mediterranean' 'Europe' 'South-East Asia'\n",
            " 'Western Pacific']\n",
            "Any missing values: year                     False\n",
            "parent_location          False\n",
            "alcohol_type             False\n",
            "average_numeric_value    False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parsing through combined_alcohol_data\n",
        "import pandas as pd\n",
        "\n",
        "# Load the combined alcohol data\n",
        "file_path = '/content/drive/MyDrive/AWS_Redshift/Alc_levels/combined_alcohol_data.csv'  # Replace with the actual path to your file\n",
        "combined_alcohol_data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract unique country names from the parent_location column\n",
        "unique_countries = combined_alcohol_data['parent_location'].unique()\n",
        "\n",
        "# Display the unique country names\n",
        "print(\"Unique Country Names in parent_location Column:\")\n",
        "print(unique_countries)\n",
        "\n",
        "# Optionally, you can save these unique country names to a text file for easier review\n",
        "with open('unique_country_names.txt', 'w') as f:\n",
        "    for country in unique_countries:\n",
        "        f.write(f\"{country}\\n\")\n",
        "\n",
        "print(\"Unique country names have been saved to 'unique_country_names.txt'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na94bhlCe2ZE",
        "outputId": "da08d94f-2d32-4bd5-af18-74e62a5f3411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Country Names in parent_location Column:\n",
            "['Africa' 'Americas' 'Eastern Mediterranean' 'Europe' 'South-East Asia'\n",
            " 'Western Pacific']\n",
            "Unique country names have been saved to 'unique_country_names.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/AWS_Redshift/Tobacco_usage.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Step 1: Remove unnecessary columns\n",
        "columns_to_remove = ['flag', 'setting_average', 'update', 'dataset_id', 'ci_lb', 'ci_ub', 'population']  # Modify this list as needed\n",
        "data_cleaned = data.drop(columns=columns_to_remove, errors='ignore')\n",
        "\n",
        "# Step 2: Rename columns to match your existing dataset structure\n",
        "data_cleaned.rename(columns={\n",
        "    'setting': 'location_name',\n",
        "    'date': 'year_id',\n",
        "    'subgroup': 'sex_name',\n",
        "    'estimate': 'smoking_prevalence_percent',  # Example column name\n",
        "}, inplace=True)\n",
        "\n",
        "# Step 3: Ensure data types are correct\n",
        "data_cleaned['year_id'] = pd.to_numeric(data_cleaned['year_id'], errors='coerce')\n",
        "data_cleaned['smoking_prevalence_percent'] = pd.to_numeric(data_cleaned['smoking_prevalence_percent'], errors='coerce')\n",
        "\n",
        "# Step 4: Handle missing values\n",
        "# Fill missing values in numeric columns with 0, or drop these rows if it's more appropriate\n",
        "data_cleaned['smoking_prevalence_percent'].fillna(0, inplace=True)\n",
        "\n",
        "# Step 5: Reorder columns to match your existing structure\n",
        "columns_order = [\n",
        "    'location_name', 'sex_name', 'year_id', 'smoking_prevalence_percent'\n",
        "    # Add any other relevant columns that should be included in this order\n",
        "]\n",
        "data_cleaned = data_cleaned[columns_order]\n",
        "averaged_data = data_cleaned.groupby(['location_name', 'year_id', 'sex_name']).agg({\n",
        "    'smoking_prevalence_percent': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Step 6: Save the cleaned and structured data to a new file\n",
        "Tobacco_usage_cleaned = '/content/drive/MyDrive/AWS_Redshift/Tobacco_usage_cleaned.xlsx'\n",
        "averaged_data.to_excel(Tobacco_usage_cleaned, index=False)\n",
        "Tobacco_usage_cleaned_noAVG = '/content/drive/MyDrive/AWS_Redshift/Tobacco_usage_cleaned_noAVG.xlsx'\n",
        "data_cleaned.to_excel(Tobacco_usage_cleaned_noAVG, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "f9ISA4okf8Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging alc_levels with lifestyle data\n",
        "import pandas as pd\n",
        "\n",
        "# Assume these are your dataset paths\n",
        "# Load the processed alcohol data (already merged if you followed the previous steps)\n",
        "alcohol_data = pd.read_csv('/content/drive/MyDrive/AWS_Redshift/Alc_levels/combined_alcohol_data.csv')\n",
        "\n",
        "# Load the lifestyle data\n",
        "lifestyle_data = pd.read_csv('/content/drive/MyDrive/AWS_Redshift/merged_lifestyle_data.csv')\n",
        "\n",
        "# Print to verify loading\n",
        "print(\"Alcohol Data Preview:\")\n",
        "print(alcohol_data.head())\n",
        "print(\"\\nLifestyle Data Preview:\")\n",
        "print(lifestyle_data.head())\n",
        "# Rename columns if needed to ensure consistency\n",
        "alcohol_data.rename(columns={'parent_location': 'country_code'}, inplace=True)\n",
        "# Ensure lifestyle data has these columns: 'year', 'country_code'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmYmsKLcpIqp",
        "outputId": "6ca8432d-06e5-448e-8597-353239446d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alcohol Data Preview:\n",
            "   Unnamed: 0  year parent_location                  alcohol_type  \\\n",
            "0           0  2000          Africa           ALCOHOLTYPE_SA_BEER   \n",
            "1           1  2000          Africa  ALCOHOLTYPE_SA_OTHER_ALCOHOL   \n",
            "2           2  2000          Africa        ALCOHOLTYPE_SA_SPIRITS   \n",
            "3           3  2000          Africa          ALCOHOLTYPE_SA_TOTAL   \n",
            "4           4  2000          Africa           ALCOHOLTYPE_SA_WINE   \n",
            "\n",
            "   average_numeric_value  \n",
            "0               0.983990  \n",
            "1               1.515731  \n",
            "2               0.545006  \n",
            "3               3.407420  \n",
            "4               0.362692  \n",
            "\n",
            "Lifestyle Data Preview:\n",
            "  location_name sex_name age_group_name  year_id  calcium_intake_g_per_day  \\\n",
            "0         China     Both       25 to 29     2000                  0.363979   \n",
            "1         China     Both       30 to 34     2000                  0.351419   \n",
            "2         China     Both       35 to 39     2000                  0.370753   \n",
            "3         China     Both       40 to 44     2000                  0.382709   \n",
            "4         China     Both       45 to 49     2000                  0.389289   \n",
            "\n",
            "   fiber_intake_g_per_day  fruit_intake_g_per_day  legumes_intake_g_per_day  \\\n",
            "0               13.013235               65.477688                 29.620451   \n",
            "1               12.361805               62.253029                 30.465697   \n",
            "2               13.194327               66.207218                 31.423916   \n",
            "3               13.489024               85.938181                 31.918052   \n",
            "4               13.940915               84.833026                 32.168174   \n",
            "\n",
            "   milk_intake_g_per_day  nuts_intake_g_per_day  omega3_intake_g_per_day  \\\n",
            "0              16.181662               3.729136                 0.226834   \n",
            "1             147.631385               3.997413                 0.228494   \n",
            "2              15.401402               4.223271                 0.231218   \n",
            "3              14.504607               4.475803                 0.238590   \n",
            "4              14.761950               4.735628                 0.249111   \n",
            "\n",
            "   processed_meat_intake_g_per_day  pufa_intake_g_per_day  \\\n",
            "0                         2.818826               1.200590   \n",
            "1                         2.794532               1.212488   \n",
            "2                         2.756746               1.234870   \n",
            "3                         2.692723               1.259717   \n",
            "4                         2.599587               1.268461   \n",
            "\n",
            "   red_meat_intake_g_per_day  sodium_intake_g_per_day  ssbs_intake_g_per_day  \\\n",
            "0                  34.319327                 7.124995              15.733011   \n",
            "1                  34.344940                 7.329786              13.167664   \n",
            "2                  34.236586                 7.627251              10.699314   \n",
            "3                  33.743263                 7.631220               8.919336   \n",
            "4                  32.628284                 7.643623               8.087337   \n",
            "\n",
            "   transfat_intake_g_per_day  veg_intake_g_per_day  \\\n",
            "0                   0.474479            256.469986   \n",
            "1                   0.478304            295.585032   \n",
            "2                   0.481786            295.893063   \n",
            "3                   0.487125            269.841359   \n",
            "4                   0.498882            276.905310   \n",
            "\n",
            "   wholegrains_intake_g_per_day  \n",
            "0                     17.763585  \n",
            "1                     19.460095  \n",
            "2                     21.145723  \n",
            "3                     22.338471  \n",
            "4                     22.796702  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Remove the unnamed column from the alcohol data if it exists\n",
        "alcohol_data = alcohol_data.loc[:, ~alcohol_data.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# Let's inspect the columns in alcohol_data to ensure the unnamed column is removed\n",
        "print(\"Alcohol Data Columns after removing unnamed column:\")\n",
        "print(alcohol_data.columns)\n",
        "\n",
        "# Step 2: Rename columns in alcohol_data to match those in lifestyle_data for merging\n",
        "alcohol_data.rename(columns={\n",
        "    'country_code': 'location_name',  # Rename country_code to location_name for consistency\n",
        "}, inplace=True)\n",
        "\n",
        "# Verify the columns again after renaming\n",
        "print(\"Updated Alcohol Data Columns after renaming:\")\n",
        "print(alcohol_data.columns)\n",
        "\n",
        "# Step 3: Pivot the alcohol data so that each alcohol type becomes a separate column\n",
        "alcohol_pivoted = alcohol_data.pivot_table(\n",
        "    index=['location_name', 'year'],  # Use the correct column names\n",
        "    columns='alcohol_type',\n",
        "    values='average_numeric_value'\n",
        ").reset_index()\n",
        "\n",
        "# Flatten the multi-level column index created by pivot_table\n",
        "alcohol_pivoted.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in alcohol_pivoted.columns]\n",
        "\n",
        "# Step 4: Merge the pivoted alcohol data with the lifestyle data\n",
        "merged_data = pd.merge(\n",
        "    lifestyle_data,   # Lifestyle data\n",
        "    alcohol_pivoted,  # Alcohol data (pivoted)\n",
        "    on=['location_name', 'year'],  # Common columns to merge on\n",
        "    how='outer'  # Use 'outer' to keep all records\n",
        ")\n",
        "\n",
        "# Print out the merged DataFrame to inspect\n",
        "print(merged_data.head())\n",
        "\n",
        "# Save the merged DataFrame to a CSV file\n",
        "merged_data.to_csv('/content/drive/MyDrive/AWS_Redshift/merged_lifestyle_data.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "_4EJHm4AxpVS",
        "outputId": "23cb59b2-0665-41e6-e1da-de545852272f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'alcohol_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-01f3d97d76ed>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Step 1: Remove the unnamed column from the alcohol data if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0malcohol_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malcohol_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0malcohol_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^Unnamed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Let's inspect the columns in alcohol_data to ensure the unnamed column is removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'alcohol_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parsing alcohol data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the combined alcohol data\n",
        "file_path = '/content/drive/MyDrive/AWS_Redshift/Alc_levels/combined_alcohol_data.csv'  # Replace with the actual path to your file\n",
        "combined_alcohol_data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the mapping from regions to genomic countries\n",
        "region_to_countries = {\n",
        "    'Africa': ['Gambia', 'Nigeria', 'Sierra Leone', 'Kenya'],  # ACB, GWD, ESN, MSL, LWK\n",
        "    'Americas': ['Puerto Rico', 'Colombia', 'Peru', 'Mexico'],  # PUR, CLM, PEL, MXL\n",
        "    'Eastern Mediterranean': ['Pakistan'],  # PJL\n",
        "    'Europe': ['United Kingdom', 'Finland', 'Spain', 'Italy'],  # GBR, FIN, IBS, TSI\n",
        "    'South-East Asia': ['India', 'Bangladesh', 'Sri Lanka'],  # ITU, GIH, BEB, STU\n",
        "    'Western Pacific': ['China', 'Japan', 'Vietnam']  # CHS, CHB, CDX, JPT, KHV\n",
        "}\n",
        "\n",
        "# Reverse the mapping to map from countries back to regions\n",
        "country_to_region = {country: region for region, countries in region_to_countries.items() for country in countries}\n",
        "\n",
        "# Function to map region to country based on the genomic countries\n",
        "def map_region_to_countries(region):\n",
        "    return region_to_countries.get(region, 'Other')\n",
        "\n",
        "# Apply the mapping to the parent_location column\n",
        "combined_alcohol_data['mapped_country'] = combined_alcohol_data['parent_location'].apply(map_region_to_countries)\n",
        "# Drop the 'Unnamed: 0' column if it exists\n",
        "if 'Unnamed: 0' in combined_alcohol_data.columns:\n",
        "    combined_alcohol_data = combined_alcohol_data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "\n",
        "# Display the first few rows to verify the mapping\n",
        "print(combined_alcohol_data.head())\n",
        "\n",
        "\n",
        "# Optionally, save the mapped data to a new CSV file\n",
        "mapped_file_path = '/content/drive/MyDrive/AWS_Redshift/Alc_levels/mapped_combined_alcohol_data.csv'\n",
        "combined_alcohol_data.to_csv(mapped_file_path, index=False)\n",
        "\n",
        "print(f\"The dataset with mapped countries has been saved to {mapped_file_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pYzNOh4-FYx",
        "outputId": "f12b6c33-75e0-4cae-f2b1-f277b466a64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year parent_location                  alcohol_type  average_numeric_value  \\\n",
            "0  2000          Africa           ALCOHOLTYPE_SA_BEER               0.983990   \n",
            "1  2000          Africa  ALCOHOLTYPE_SA_OTHER_ALCOHOL               1.515731   \n",
            "2  2000          Africa        ALCOHOLTYPE_SA_SPIRITS               0.545006   \n",
            "3  2000          Africa          ALCOHOLTYPE_SA_TOTAL               3.407420   \n",
            "4  2000          Africa           ALCOHOLTYPE_SA_WINE               0.362692   \n",
            "\n",
            "                           mapped_country  \n",
            "0  [Gambia, Nigeria, Sierra Leone, Kenya]  \n",
            "1  [Gambia, Nigeria, Sierra Leone, Kenya]  \n",
            "2  [Gambia, Nigeria, Sierra Leone, Kenya]  \n",
            "3  [Gambia, Nigeria, Sierra Leone, Kenya]  \n",
            "4  [Gambia, Nigeria, Sierra Leone, Kenya]  \n",
            "The dataset with mapped countries has been saved to /content/drive/MyDrive/AWS_Redshift/Alc_levels/mapped_combined_alcohol_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the mapping of genomic country codes to full country names\n",
        "country_code_to_name = {\n",
        "    'GBR': 'United Kingdom',\n",
        "    'FIN': 'Finland',\n",
        "    'CHS': 'China',\n",
        "    'PUR': 'Puerto Rico',\n",
        "    'CDX': 'China',\n",
        "    'CLM': 'Colombia',\n",
        "    'IBS': 'Spain',\n",
        "    'PEL': 'Peru',\n",
        "    'PJL': 'Pakistan',\n",
        "    'KHV': 'Vietnam',\n",
        "    'ACB': 'Caribbean',\n",
        "    'GWD': 'Gambia',\n",
        "    'ESN': 'Nigeria',\n",
        "    'BEB': 'Bangladesh',\n",
        "    'MSL': 'Sierra Leone',\n",
        "    'STU': 'Sri Lanka',\n",
        "    'ITU': 'India',\n",
        "    'CEU': 'United States',\n",
        "    'YRI': 'Nigeria',\n",
        "    'CHB': 'China',\n",
        "    'JPT': 'Japan',\n",
        "    'LWK': 'Kenya',\n",
        "    'ASW': 'African American',\n",
        "    'MXL': 'Mexico',\n",
        "    'TSI': 'Italy',\n",
        "    'GIH': 'India'\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "w9HCQ54Ncc93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}